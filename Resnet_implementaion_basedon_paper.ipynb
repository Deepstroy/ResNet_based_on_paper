{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_good.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si0-ZmLUG-7W",
        "colab_type": "code",
        "outputId": "6386da20-8e99-4bf4-cecb-80745ba46a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "!pip install tensorboardcolab\n",
        "import tensorboardcolab\n",
        "\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYKlEpkV3S9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(100000):\n",
        "#     sys.stdout.write('/r progress : {}\\n'.format(i))\n",
        "#     sys.stdout.flush()\n",
        "#     if i == 1000 :\n",
        "#         print('/n hi')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBu_McydG-75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "(train_x, train_y), (test_x, test_y) = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dyGFuErOZ7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_y = test_y.reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgWK_zkRgXJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "    # 데이터셋을 배치 단위로 처리할 수 있도록 도와주는 Class\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images.copy()\n",
        "        self.labels = labels.copy()\n",
        "        self.labels = self.labels.ravel() # (None,1) -> (None,)\n",
        "        self.counter = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        # 데이터의 갯수를 반환\n",
        "        # e.g) len(dataset)\n",
        "        return len(self.images)\n",
        "\n",
        "    def next_batch(self, batch_size=32):\n",
        "        if self.counter + batch_size > len(self.images):\n",
        "            # counter가 데이터셋의 갯수보다 초과한 경우,\n",
        "            # 데이터셋을 섞어줌\n",
        "            self.shuffle()\n",
        "            self.counter = 0\n",
        "        batch_images = self.images[self.counter:self.counter+batch_size]\n",
        "        batch_labels = self.labels[self.counter:self.counter+batch_size]\n",
        "        self.counter+=batch_size\n",
        "        return batch_images.copy(), batch_labels.copy()\n",
        "    \n",
        "    def shuffle(self):\n",
        "        # 데이터 셋을 섞어주는 함수\n",
        "        indices = np.arange(len(self.images))\n",
        "        np.random.shuffle(indices)        \n",
        "        self.images = self.images[indices]\n",
        "        self.labels = self.labels[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqbz0WiYm51s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm_cnn (inputs, is_train) :\n",
        "    # Eps, decay\n",
        "    epsilon = 1e-5\n",
        "    decay = 0.9999\n",
        "    \n",
        "    #input shape의 feature size측정\n",
        "    sizes = inputs.get_shape()[-1]\n",
        "    \n",
        "    #test를 위한 moving average를 계산위함, 초기값은 0으로 지정 : tf.zeros\n",
        "    test_mean = tf.Variable(tf.zeros([sizes]), name = 'test_mean')\n",
        "    test_var = tf.Variable(tf.zeros([sizes]), name = 'test_var')\n",
        "    \n",
        "    # featuare갯수만큼 scale factor와 shiftf factor\n",
        "    gamma = tf.Variable(tf.ones(sizes), name='scale_factor')\n",
        "    beta = tf.Variable(tf.zeros(sizes), name='shift_factor')\n",
        "\n",
        "    def train():\n",
        "        mean, var = tf.nn.moments(inputs, [0, 1, 2])\n",
        "        updated_mean = tf.assign(test_mean, \n",
        "                                 (((1-decay) * mean) + (decay*(test_mean))))\n",
        "        updated_var = tf.assign(test_var,\n",
        "                                (((1-decay) * var) + (decay*(test_var))))\n",
        "        \n",
        "        with tf.control_dependencies([updated_mean ,updated_var]) :\n",
        "            xs_bn = tf.nn.batch_normalization(inputs, mean, var, \n",
        "                                              beta, gamma, epsilon)\n",
        "            \n",
        "            #xs_norm = (xs - mean) /tf.sqrt(var+eps)\n",
        "            #xs_bn = (xs_norm * gamma) + beta\n",
        "            \n",
        "            return xs_bn    \n",
        "    \n",
        "    def test():\n",
        "        #test할때나 train할때나 beta gamma는 동일함\n",
        "        #xs_norm = (xs - test_mean) / tf.sqrt(test_var+eps)\n",
        "        #xs_bn = (xs_norm*gamma) + beta\n",
        "        #high api사용\n",
        "        xs_bn = tf.nn.batch_normalization(inputs, test_mean, test_var,\n",
        "                                          beta, gamma, epsilon)\n",
        "        \n",
        "        return xs_bn\n",
        "        \n",
        "    xs_bn = tf.cond(is_train, train, test)\n",
        "    \n",
        "    return xs_bn\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8WTY9qufVcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augmentaion(xss, is_train) :\n",
        "    def train(xs) :\n",
        "        #xs = tf.image.resize_image_with_crop_or_pad(xs, )\n",
        "        xs = tf.image.flip_left_right(xs)\n",
        "        xs = tf.image.random_brightness(xs, 0.8)\n",
        "        xs = tf.image.random_flip_up_down(xs)\n",
        "        #xs = tf.image.adjust_contrast(xs, )\n",
        "        xs = tf.image.random_saturation(xs, 0.5 , 1.8)\n",
        "        xs = tf.image.per_image_standardization(xs)\n",
        "        return xs\n",
        "        \n",
        "    def test(xs) :\n",
        "        xs = tf.image.per_image_standardization(xs)\n",
        "        return xs\n",
        "    \n",
        "    \n",
        "    if is_train == True :\n",
        "        xs = tf.map_fn(lambda xs : train(xs), xss)\n",
        "    else :\n",
        "        xs = tf.map_fn(lambda xs : test(xs), xss)\n",
        "    return xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h67HQ3hbhayp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(input_data, filters, strides, is_train, block_name) :\n",
        "    #print('input_shape : ', input_data.shape)\n",
        "    \n",
        "    he_init = tf.initializers.he_normal()\n",
        "    if input_data.shape[-1] != filters :\n",
        "        projections = tf.layers.Conv2D(filters = filters, kernel_size = 1, \n",
        "                                      strides = 1, padding='SAME',\n",
        "                                      kernel_initializer = he_init)(input_data)\n",
        "    else :\n",
        "        projections = input_data\n",
        "        \n",
        "    with tf.variable_scope(block_name) :\n",
        "        con = tf.layers.Conv2D(filters=filters, kernel_size = 3,\n",
        "                               strides=strides, padding = 'SAME',\n",
        "                               kernel_initializer = he_init)(projections)\n",
        "        bn = tf.layers.BatchNormalization()(con, training=is_train)\n",
        "        #bn = batch_norm_cnn(con, is_train)\n",
        "        act = tf.nn.relu(bn)\n",
        "        \n",
        "        con = tf.layers.Conv2D(filters = filters, kernel_size = 3,\n",
        "                               strides = strides, padding = 'SAME',\n",
        "                               kernel_initializer = he_init)\\\n",
        "                               (act)\n",
        "        bn = tf.layers.BatchNormalization()(con, training=is_train)\n",
        "        #bn = batch_norm_cnn(con, is_train)\n",
        "\n",
        "        # Skip-connection layer\n",
        "        sum_ = tf.add(bn ,input_data)\n",
        "        \n",
        "        # activation\n",
        "        out_data = tf.nn.relu(sum_)\n",
        "    return out_data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajM0u5PKfPoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "d7a3aba5-2f60-434a-ea76-73b0574147d5"
      },
      "source": [
        "graph1 = tf.Graph()\n",
        "with graph1.as_default() :\n",
        "    xs = tf.placeholder(dtype = tf.float32, shape=(None, 32, 32, 3))\n",
        "    ys = tf.placeholder(dtype = tf.int64, shape=(None,))\n",
        "    is_train = tf.placeholder_with_default(True, (), name='is_train')\n",
        "    \n",
        "    xs = augmentaion(xs, is_train)\n",
        "    \n",
        "    #first_out = tf.layers.Conv2D(filters = 16, kernel_size = 7, strides = 2,\n",
        "    #                               padding = 'SAME')\n",
        "    xa_init = tf.initializers.glorot_normal()\n",
        "    he_init = tf.initializers.he_normal()\n",
        "\n",
        "    for i in range(1,5) :\n",
        "        if i == 1:\n",
        "            #conv stride=2로 이미지 사이즈를 반으로 줄임\n",
        "            out = tf.layers.Conv2D(filters = 16, kernel_size = 3, strides = 2,\n",
        "                                   padding = 'SAME', name= 'conv2_first')(xs)\n",
        "        else :\n",
        "            out = residual_block(out, 16, 1, is_train,'conv2_{}'.format(i))\n",
        "            \n",
        "    for i in range(1,4) :\n",
        "        if i == 1:\n",
        "            out = tf.layers.Conv2D(filters = 32, kernel_size = 3, strides = 2,\n",
        "                                   padding = 'SAME', name= 'conv3_first')(out)\n",
        "        else :\n",
        "            out = residual_block(out, 32, 1, is_train,'conv3_{}'.format(i))\n",
        "    \n",
        "    for i in range(1,4) :\n",
        "        if i == 1:\n",
        "            out = tf.layers.Conv2D(filters = 64, kernel_size = 3, strides = 2,\n",
        "                                   padding = 'SAME', name= 'conv4_first')(out)\n",
        "        else :\n",
        "            out = residual_block(out, 64, 1, is_train,'conv4_{}'.format(i))\n",
        "            \n",
        "    with tf.variable_scope('global_average_pool') :\n",
        "        # [none, w, h, feature] 에서 w,h에 대해 평균 => image size에 유연함\n",
        "        gap = tf.reduce_mean(out, axis=(1,2))\n",
        "        \n",
        "    logits = tf.layers.Dense(units = 10,\n",
        "                             activation = None,\n",
        "                             use_bias = True,\n",
        "                             kernel_initializer = tf.initializers.glorot_normal())(gap)\n",
        "    y_pred = tf.nn.softmax(logits)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0704 11:21:33.447066 140635668719488 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0704 11:21:33.465213 140635668719488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ELye8-LffSgE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "9872b954-4a31-43fa-c291-09c04f638775"
      },
      "source": [
        " with graph1.as_default() :\n",
        "    with tf.variable_scope('loss') :\n",
        "        weight_decay = 1e-4\n",
        "        sce = tf.losses.sparse_softmax_cross_entropy(labels = ys,\n",
        "                                                      logits = logits)\n",
        "        \n",
        "        weights = graph1.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                       scope='^.*/kernel.*$')\n",
        "        l2_loss = tf.add_n([tf.nn.l2_loss(weight) \n",
        "                            for weight in weights])\n",
        "        \n",
        "        loss = sce + weight_decay * l2_loss\n",
        "    loss = tf.identity(loss, name='loss')  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0704 11:21:34.938717 140635668719488 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RriF1xjmipuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with graph1.as_default() :\n",
        "    with tf.variable_scope('metric_acc') :\n",
        "        pred_val = tf.argmax(y_pred,axis=1)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(pred_val, ys), tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__saIbt2S70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with graph1.as_default() :\n",
        "    momentum = 0.9\n",
        "    lr = tf.placeholder(tf.float32, (), name='learning_rate')    \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    with tf.variable_scope(\"optimizer\"):\n",
        "        update_ops = graph1.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        # update_ops는 moving 계산하는 opration임 -> train_op실행\n",
        "        # 여기서 train과 test를 control 해줌\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            train_op = (tf.train.AdamOptimizer(lr)\n",
        "                    .minimize(loss, global_step=global_step))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CORSLo--fTpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d8ca117-5493-4e15-9a40-d67888d5e6ec"
      },
      "source": [
        "with graph1.as_default() :\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    provider = Dataset(train_x, train_y)\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "    loss_val_list = []\n",
        "    acc_val_list = []\n",
        "    \n",
        "    for i in range(10000) :\n",
        "        data, label = provider.next_batch(100)\n",
        "        _, loss_, acc_ = sess.run([train_op, loss, acc],\n",
        "                                              feed_dict = { xs : data,\n",
        "                                                           ys : label,\n",
        "                                                          is_train : True,\n",
        "                                                          lr : 1e-3 })\n",
        "        \n",
        "        if i % 100 == 0 :\n",
        "            loss_val, acc_val = sess.run([loss, acc],\n",
        "                                              feed_dict = { xs : test_x,\n",
        "                                                           ys : test_y,\n",
        "                                                          is_train : False})\n",
        "            loss_list.append(loss_)\n",
        "            acc_list.append(acc_)\n",
        "            loss_val_list.append(loss_val)\n",
        "            acc_val_list.append(acc_val)\n",
        "            \n",
        "            print('step : {}, train_loss : {:5.3}, train_acc : {:5.3}, vali_loss : {:5.3}, vali_acc : {:5.3}'.format(i,loss_, acc_, loss_val, acc_val))\n",
        "            "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 0, train_loss :  21.9, train_acc :  0.06, vali_loss :  37.9, vali_acc : 0.118\n",
            "step : 100, train_loss :  2.13, train_acc :  0.26, vali_loss :  2.39, vali_acc :  0.21\n",
            "step : 200, train_loss :  2.02, train_acc :  0.34, vali_loss :  2.11, vali_acc : 0.268\n",
            "step : 300, train_loss :  1.94, train_acc :  0.37, vali_loss :  1.95, vali_acc : 0.318\n",
            "step : 400, train_loss :  1.71, train_acc :  0.43, vali_loss :  1.91, vali_acc : 0.329\n",
            "step : 500, train_loss :  1.75, train_acc :  0.41, vali_loss :  1.83, vali_acc : 0.362\n",
            "step : 600, train_loss :  1.68, train_acc :  0.35, vali_loss :  1.78, vali_acc : 0.371\n",
            "step : 700, train_loss :  1.54, train_acc :  0.47, vali_loss :  1.72, vali_acc : 0.398\n",
            "step : 800, train_loss :  1.67, train_acc :  0.45, vali_loss :  1.73, vali_acc : 0.395\n",
            "step : 900, train_loss :  1.65, train_acc :  0.44, vali_loss :  1.63, vali_acc : 0.427\n",
            "step : 1000, train_loss :  1.47, train_acc :  0.53, vali_loss :  1.63, vali_acc : 0.423\n",
            "step : 1100, train_loss :  1.52, train_acc :  0.48, vali_loss :  1.61, vali_acc : 0.437\n",
            "step : 1200, train_loss :  1.63, train_acc :  0.43, vali_loss :  1.59, vali_acc : 0.445\n",
            "step : 1300, train_loss :  1.71, train_acc :  0.39, vali_loss :   1.6, vali_acc : 0.433\n",
            "step : 1400, train_loss :  1.55, train_acc :  0.47, vali_loss :  1.53, vali_acc : 0.465\n",
            "step : 1500, train_loss :  1.57, train_acc :  0.43, vali_loss :  1.59, vali_acc :  0.44\n",
            "step : 1600, train_loss :   1.6, train_acc :  0.41, vali_loss :  1.51, vali_acc : 0.473\n",
            "step : 1700, train_loss :  1.45, train_acc :  0.52, vali_loss :  1.54, vali_acc :  0.46\n",
            "step : 1800, train_loss :  1.53, train_acc :  0.52, vali_loss :  1.49, vali_acc : 0.477\n",
            "step : 1900, train_loss :  1.64, train_acc :  0.46, vali_loss :  1.54, vali_acc :  0.46\n",
            "step : 2000, train_loss :  1.45, train_acc :  0.49, vali_loss :  1.48, vali_acc : 0.473\n",
            "step : 2100, train_loss :  1.27, train_acc :  0.57, vali_loss :   1.5, vali_acc : 0.468\n",
            "step : 2200, train_loss :  1.38, train_acc :  0.52, vali_loss :  1.45, vali_acc : 0.491\n",
            "step : 2300, train_loss :  1.25, train_acc :  0.58, vali_loss :  1.43, vali_acc : 0.498\n",
            "step : 2400, train_loss :   1.3, train_acc :  0.51, vali_loss :  1.43, vali_acc :   0.5\n",
            "step : 2500, train_loss :  1.39, train_acc :   0.5, vali_loss :  1.42, vali_acc : 0.503\n",
            "step : 2600, train_loss :  1.46, train_acc :  0.46, vali_loss :  1.44, vali_acc : 0.493\n",
            "step : 2700, train_loss :  1.39, train_acc :  0.51, vali_loss :   1.4, vali_acc : 0.508\n",
            "step : 2800, train_loss :  1.48, train_acc :  0.47, vali_loss :  1.53, vali_acc : 0.462\n",
            "step : 2900, train_loss :  1.25, train_acc :  0.56, vali_loss :  1.41, vali_acc : 0.504\n",
            "step : 3000, train_loss :  1.21, train_acc :   0.6, vali_loss :  1.39, vali_acc : 0.512\n",
            "step : 3100, train_loss :  1.23, train_acc :  0.57, vali_loss :  1.37, vali_acc : 0.518\n",
            "step : 3200, train_loss :  1.15, train_acc :  0.54, vali_loss :  1.38, vali_acc : 0.518\n",
            "step : 3300, train_loss :  1.51, train_acc :  0.54, vali_loss :  1.36, vali_acc : 0.524\n",
            "step : 3400, train_loss :  1.33, train_acc :   0.6, vali_loss :  1.38, vali_acc : 0.518\n",
            "step : 3500, train_loss :  1.25, train_acc :  0.52, vali_loss :  1.36, vali_acc : 0.527\n",
            "step : 3600, train_loss :  1.26, train_acc :  0.62, vali_loss :  1.33, vali_acc : 0.541\n",
            "step : 3700, train_loss :  1.18, train_acc :  0.56, vali_loss :  1.32, vali_acc : 0.538\n",
            "step : 3800, train_loss :   1.0, train_acc :  0.68, vali_loss :  1.33, vali_acc :  0.54\n",
            "step : 3900, train_loss :  1.21, train_acc :  0.59, vali_loss :  1.33, vali_acc : 0.542\n",
            "step : 4000, train_loss :  1.32, train_acc :  0.54, vali_loss :  1.33, vali_acc : 0.539\n",
            "step : 4100, train_loss :  1.19, train_acc :  0.64, vali_loss :  1.34, vali_acc : 0.536\n",
            "step : 4200, train_loss :  1.09, train_acc :  0.63, vali_loss :  1.33, vali_acc : 0.541\n",
            "step : 4300, train_loss :  1.21, train_acc :  0.53, vali_loss :   1.3, vali_acc : 0.549\n",
            "step : 4400, train_loss :  1.17, train_acc :  0.58, vali_loss :   1.3, vali_acc :  0.55\n",
            "step : 4500, train_loss :  1.14, train_acc :  0.62, vali_loss :  1.31, vali_acc : 0.552\n",
            "step : 4600, train_loss :   1.2, train_acc :  0.61, vali_loss :  1.34, vali_acc : 0.536\n",
            "step : 4700, train_loss :  1.19, train_acc :  0.57, vali_loss :  1.27, vali_acc : 0.565\n",
            "step : 4800, train_loss :   1.2, train_acc :  0.57, vali_loss :   1.3, vali_acc : 0.555\n",
            "step : 4900, train_loss :  1.23, train_acc :  0.58, vali_loss :  1.34, vali_acc :  0.54\n",
            "step : 5000, train_loss :  1.13, train_acc :  0.57, vali_loss :  1.29, vali_acc : 0.557\n",
            "step : 5100, train_loss :  0.98, train_acc :  0.72, vali_loss :  1.28, vali_acc : 0.562\n",
            "step : 5200, train_loss :   1.0, train_acc :  0.68, vali_loss :  1.41, vali_acc : 0.513\n",
            "step : 5300, train_loss :  1.38, train_acc :  0.52, vali_loss :  1.27, vali_acc : 0.562\n",
            "step : 5400, train_loss : 0.999, train_acc :  0.69, vali_loss :   1.3, vali_acc : 0.555\n",
            "step : 5500, train_loss :  1.04, train_acc :  0.66, vali_loss :  1.24, vali_acc : 0.578\n",
            "step : 5600, train_loss :  1.08, train_acc :  0.64, vali_loss :  1.23, vali_acc : 0.577\n",
            "step : 5700, train_loss :  0.95, train_acc :  0.72, vali_loss :  1.24, vali_acc : 0.571\n",
            "step : 5800, train_loss :  1.01, train_acc :   0.6, vali_loss :  1.26, vali_acc : 0.562\n",
            "step : 5900, train_loss :  1.18, train_acc :  0.61, vali_loss :  1.24, vali_acc : 0.575\n",
            "step : 6000, train_loss : 0.991, train_acc :   0.7, vali_loss :  1.28, vali_acc : 0.557\n",
            "step : 6100, train_loss : 0.945, train_acc :  0.73, vali_loss :  1.25, vali_acc : 0.574\n",
            "step : 6200, train_loss : 0.947, train_acc :   0.7, vali_loss :  1.24, vali_acc :  0.58\n",
            "step : 6300, train_loss : 0.903, train_acc :  0.67, vali_loss :   1.3, vali_acc : 0.555\n",
            "step : 6400, train_loss :  1.06, train_acc :  0.67, vali_loss :  1.25, vali_acc : 0.569\n",
            "step : 6500, train_loss :  1.06, train_acc :  0.67, vali_loss :  1.22, vali_acc : 0.578\n",
            "step : 6600, train_loss : 0.949, train_acc :  0.68, vali_loss :  1.39, vali_acc : 0.546\n",
            "step : 6700, train_loss :  1.06, train_acc :  0.64, vali_loss :  1.24, vali_acc : 0.581\n",
            "step : 6800, train_loss :  1.13, train_acc :  0.69, vali_loss :  1.21, vali_acc : 0.591\n",
            "step : 6900, train_loss : 0.949, train_acc :   0.7, vali_loss :  1.29, vali_acc : 0.569\n",
            "step : 7000, train_loss :  1.09, train_acc :  0.58, vali_loss :  1.25, vali_acc : 0.576\n",
            "step : 7100, train_loss : 0.954, train_acc :  0.72, vali_loss :  1.21, vali_acc : 0.589\n",
            "step : 7200, train_loss : 0.847, train_acc :  0.74, vali_loss :  1.23, vali_acc : 0.578\n",
            "step : 7300, train_loss : 0.959, train_acc :  0.69, vali_loss :  1.24, vali_acc : 0.582\n",
            "step : 7400, train_loss : 0.865, train_acc :  0.75, vali_loss :  1.23, vali_acc : 0.584\n",
            "step : 7500, train_loss : 0.882, train_acc :  0.68, vali_loss :  1.22, vali_acc : 0.591\n",
            "step : 7600, train_loss : 0.923, train_acc :  0.74, vali_loss :  1.24, vali_acc : 0.581\n",
            "step : 7700, train_loss :   1.0, train_acc :  0.67, vali_loss :  1.18, vali_acc : 0.601\n",
            "step : 7800, train_loss : 0.964, train_acc :  0.71, vali_loss :  1.25, vali_acc :  0.58\n",
            "step : 7900, train_loss : 0.954, train_acc :  0.72, vali_loss :  1.33, vali_acc : 0.561\n",
            "step : 8000, train_loss : 0.751, train_acc :  0.76, vali_loss :  1.38, vali_acc : 0.542\n",
            "step : 8100, train_loss :  0.94, train_acc :  0.65, vali_loss :  1.18, vali_acc : 0.603\n",
            "step : 8200, train_loss :  0.78, train_acc :  0.77, vali_loss :  1.24, vali_acc : 0.589\n",
            "step : 8300, train_loss : 0.869, train_acc :  0.73, vali_loss :   1.2, vali_acc : 0.598\n",
            "step : 8400, train_loss : 0.888, train_acc :  0.69, vali_loss :  1.21, vali_acc : 0.593\n",
            "step : 8500, train_loss : 0.978, train_acc :  0.68, vali_loss :  1.19, vali_acc : 0.595\n",
            "step : 8600, train_loss : 0.926, train_acc :  0.72, vali_loss :  1.23, vali_acc : 0.586\n",
            "step : 8700, train_loss : 0.867, train_acc :  0.74, vali_loss :   1.2, vali_acc : 0.599\n",
            "step : 8800, train_loss :  1.13, train_acc :   0.6, vali_loss :  1.27, vali_acc : 0.572\n",
            "step : 8900, train_loss : 0.963, train_acc :  0.73, vali_loss :  1.21, vali_acc : 0.599\n",
            "step : 9000, train_loss : 0.699, train_acc :  0.82, vali_loss :  1.21, vali_acc : 0.595\n",
            "step : 9100, train_loss : 0.816, train_acc :  0.79, vali_loss :  1.16, vali_acc : 0.615\n",
            "step : 9200, train_loss : 0.648, train_acc :  0.81, vali_loss :  1.27, vali_acc : 0.578\n",
            "step : 9300, train_loss : 0.821, train_acc :   0.7, vali_loss :  1.23, vali_acc : 0.595\n",
            "step : 9400, train_loss :   1.0, train_acc :  0.69, vali_loss :   1.2, vali_acc : 0.603\n",
            "step : 9500, train_loss : 0.868, train_acc :  0.74, vali_loss :  1.22, vali_acc :   0.6\n",
            "step : 9600, train_loss : 0.606, train_acc :  0.83, vali_loss :  1.18, vali_acc : 0.608\n",
            "step : 9700, train_loss : 0.841, train_acc :  0.72, vali_loss :  1.23, vali_acc : 0.594\n",
            "step : 9800, train_loss : 0.833, train_acc :  0.72, vali_loss :  1.18, vali_acc : 0.605\n",
            "step : 9900, train_loss : 0.877, train_acc :  0.71, vali_loss :  1.18, vali_acc : 0.609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-8SEqP9gtNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "3bdb10c1-0a62-4c02-dccb-e2922123f26e"
      },
      "source": [
        "with graph1.as_default() :\n",
        "    for i in range(10000,15000) :\n",
        "        # Learning rate => 1/10 decay\n",
        "        data, label = provider.next_batch(100)\n",
        "        _, loss_, acc_ = sess.run([train_op, loss, acc],\n",
        "                                              feed_dict = { xs : data,\n",
        "                                                           ys : label,\n",
        "                                                          is_train : True,\n",
        "                                                          lr : 1e-5 })\n",
        "        \n",
        "        if i % 1000 == 0 :\n",
        "            loss_val, acc_val = sess.run([loss, acc],\n",
        "                                              feed_dict = { xs : test_x,\n",
        "                                                           ys : test_y,\n",
        "                                                          is_train : False})\n",
        "            loss_list.append(loss_)\n",
        "            acc_list.append(acc_)\n",
        "            loss_val_list.append(loss_val)\n",
        "            acc_val_list.append(acc_val)\n",
        "            \n",
        "            print('step : {}, train_loss : {:5.3}, train_acc : {:5.3}, vali_loss : {:5.3}, vali_acc : {:5.3}'.format(i,loss_, acc_, loss_val, acc_val))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 10000, train_loss : 0.725, train_acc :  0.74, vali_loss :  1.19, vali_acc : 0.606\n",
            "step : 11000, train_loss : 0.648, train_acc :  0.78, vali_loss :  1.14, vali_acc : 0.622\n",
            "step : 12000, train_loss : 0.806, train_acc :   0.8, vali_loss :  1.14, vali_acc : 0.624\n",
            "step : 13000, train_loss : 0.728, train_acc :  0.77, vali_loss :  1.14, vali_acc : 0.626\n",
            "step : 14000, train_loss : 0.804, train_acc :   0.8, vali_loss :  1.14, vali_acc : 0.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd3DH4BBi-GC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "28cefda5-c440-4eaa-a1c2-3a8989ae6016"
      },
      "source": [
        "with graph1.as_default() :\n",
        "    for i in range(15000,20000) :\n",
        "        # Learning rate => 1/10 decay\n",
        "        data, label = provider.next_batch(100)\n",
        "        _, loss_, acc_ = sess.run([train_op, loss, acc],\n",
        "                                              feed_dict = { xs : data,\n",
        "                                                           ys : label,\n",
        "                                                          is_train : True,\n",
        "                                                          lr : 1e-5 })\n",
        "        \n",
        "        if i % 1000 == 0 :\n",
        "            loss_val, acc_val = sess.run([loss, acc],\n",
        "                                              feed_dict = { xs : test_x,\n",
        "                                                           ys : test_y,\n",
        "                                                          is_train : False})\n",
        "            loss_list.append(loss_)\n",
        "            acc_list.append(acc_)\n",
        "            loss_val_list.append(loss_val)\n",
        "            acc_val_list.append(acc_val)\n",
        "            \n",
        "            print('step : {}, train_loss : {:5.3}, train_acc : {:5.3}, vali_loss : {:5.3}, vali_acc : {:5.3}'.format(i,loss_, acc_, loss_val, acc_val))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 15000, train_loss : 0.856, train_acc :   0.7, vali_loss :  1.14, vali_acc : 0.623\n",
            "step : 16000, train_loss : 0.735, train_acc :  0.78, vali_loss :  1.15, vali_acc : 0.623\n",
            "step : 17000, train_loss : 0.668, train_acc :  0.79, vali_loss :  1.15, vali_acc : 0.625\n",
            "step : 18000, train_loss : 0.532, train_acc :  0.85, vali_loss :  1.15, vali_acc : 0.627\n",
            "step : 19000, train_loss : 0.774, train_acc :  0.74, vali_loss :  1.15, vali_acc : 0.627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t23dr6DvOvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}